## Encoder models
[![Encoder models]
(https://img.youtube.com/vi/MUqNwgPjJvQ/0.jpg)]
(https://youtu.be/MUqNwgPjJvQ)
Encoder models in the Transformer architecture focus on utilizing the encoder part of the model. They employ "bi-directional" attention, enabling them to access all words in the input sentence simultaneously. These models are often referred to as auto-encoding models.

The pretraining process for encoder models involves corrupting a given sentence, for example, by masking random words, and then tasking the model with the reconstruction of the original sentence.

Encoder models are particularly well-suited for tasks that require understanding the full context of a sentence. They excel in tasks such as sentence classification, named entity recognition, word classification, and extractive question answering.

In summary, encoder models leverage bi-directional attention to process input sequences and are trained through auto-encoding tasks. They are powerful tools for tasks that demand a comprehensive understanding of the entire sentence context.

- BERT
- ALBERT
- RoBERTa
- ELECTRA
  
### Encoder-only models are good for tasks that require understanding of the input, such as:
- Sentence classification
- Named entity recognition







### Further reading:
[![from start](https://img.youtube.com/vi/0.jpg)](https://youtu.be/CHFiTTPeyUw)



